---
repo: fighting41love/funNLP
readme_filename: fighting41love_funNLP_README.md
stars: 74401
forks: 14890
watchers: 74401
contributors_count: 11
license: None
Header 1: 类ChatGPT的资料
---
| 资源名（Name）      | 描述（Description） | 链接     |
| :---        |    :----   |          :--- |
|  Open LLMs：可供商业使用的开放大型语言模型(LLM)    |    A list of open LLMs available for commercial use   |   github    |
|LLM Zoo: 大型语言模型的数据、模型和基准集市|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|github|
|   大型语言模型(LLM)资料合集   |     相关论文列表，包括指导、推理、决策、持续改进和自我提升等方面的研究工作  |    LLM资料合集   |
|DecryptPrompt|总结Prompt&LLM论文，开源数据&模型，AIGC应用|github|
|   SmartGPT    |   旨在为大型语言模型(尤其是GPT-3.5和GPT-4)提供完成复杂任务的能力，通过将它们分解成更小的问题，并使用互联网和其他外部来源收集信息。特点包括模块化设计，易于配置，以及对插件的高度支持。SmartGPT的运作基于"Autos"的概念，包括"Runner"和"Assistant"两种类型，都配有处理计划、推理和任务执行的LLM代理。此外，SmartGPT还具有内存管理系统，以及可以定义各种命令的插件系统     |  github-SmartGPT  |
| OpenGPT      |   用于创建基于指令的数据集并训练对话领域专家大型语言模型(LLMs)的框架。已经成功应用于训练健康护理对话模型NHS-LLM，利用来自英国国家卫生服务体系(NHS)网站的数据，生成了大量的问答对和独特对话  |          github-OpenGPT |
|   PaLM 2技术报告   |   Google最新发布PaLM 2，一种新的语言模型，具有更好的多语言和推理能力，同时比其前身PaLM更节省计算资源。PaLM 2综合了多项研究进展，包括计算最优的模型和数据规模、更多样化和多语言的数据集、以及更有效的模型架构和目标函数。PaLM 2在多种任务和能力上达到了最先进的性能，包括语言水平考试、分类和问答、推理、编程、翻译和自然语言生成等。PaLM 2还展示了强大的多语言能力，能够处理数百种语言，并在不同语言之间进行翻译和解释。PaLM 2还考虑了负责任的使用问题，包括推理时控制毒性、减少记忆化、评估潜在的伤害和偏见等    |    PaLM 2 Technical Report   |
|   DB-GPT   |    于vicuna-13b和FastChat的开源实验项目，采用了langchain和llama-index技术进行上下文学习和问答。项目完全本地化部署，保证数据的隐私安全，能直接连接到私有数据库处理私有数据。其功能包括SQL生成、SQL诊断、数据库知识问答等   |   github-DB-GPT     |
|  Transformers相关文献资源大列表    |   包含了各种各样的Transformer模型，例如BERT、GPT、Transformer-XL等，这些模型已经在许多自然语言处理任务中得到了广泛应用。此外，该列表还提供了这些模型的相关论文和代码链接，为自然语言处理领域的研究人员和开发者提供了很好的参考资源    |   github    |
|   GPT-4终极指南   |    一份关于如何使用GPT3和GPT4的指南，其中包括100多个资源，可以帮助学习如何用它来提高生活效率。包括如何学习ChatGPT基础知识、如何学习ChatGPT高级知识、如何在语言学习中使用GPT-3、如何在教学中使用GPT-3、如何使用GPT-4等，还提供了如何升级到ChatGPT+计划以使用GPT-4以及如何免费使用GPT-4的方法等内容。同时，还提供了如何在业务、生产力、受益、金钱等方面使用ChatGPT的指南   |   link    |
|  基于LoRA的LLM参数高效微调    |       |   link    |
|  复杂推理：大语言模型的北极星能力     |  在 GPT-4 发布博客中，作者写道：“在一次随意的谈话中，GPT-3.5 和 GPT-4 之间的区别可能是微妙的。当任务的复杂程度达到足够的阈值时，差异就会显现出来。”这意味着复杂任务很可能是大型和小型语言模型的关键差异因素。在这篇文章中，我们将仔细分析讨论如何让大语言模型拥有强大的复杂推理能力。     |   blog    |
|   大型语言模型的涌现能力是否是海市蜃楼？   |    大语言模型的涌现能力一直是被大家视作很神奇的现象，似乎是一种大力出奇迹，但这篇论文认为这可能只是一种错觉。   |   paper    |
|   大语言模型的概率总结  |   非常详尽的LLM科学解释和总结    |   paper    |
|  LLaMA 模型简史    |    LLaMA是Meta发布的语言模型，采用Transformer架构，有多个版本，最大为65B参数。与GPT类似，可用于进一步微调，适用于多种任务。与GPT不同的是，LLaMA是开源的，可以在本地运行。现有的LLaMA模型包括：Alpaca、Vicuna、Koala、GPT4-x-Alpaca和WizardLM。每个模型都有不同的训练数据和性能表现   |   blog    |
|  大型语言模型的复杂推理     |   讨论了如何训练具有强大复杂推理能力的语言模型，并探讨了如何有效地提示模型以充分释放其潜力；针对语言模型和编程的训练相似性，提出了三阶段的训练：持续训练、监督微调和强化学习；介绍了评估大型语言模型推理能力的一套任务集合；讨论了如何进行提示工程，通过提供各种学习机会使模型获得更好的学习效果，最终实现智能化    |   link    |
|   大语言模型进化树   |       |    paper   |
|李宏毅：穷人如何低资源复刻自己的ChatGPT||blog|
|   训练ChatGPT的必备资源：语料、模型和代码库完全指南   |       |    资源链接论文地址   |
|  GitHub宝藏库，里面整理了GPT相关的各种开源项目    |       |    github   |
|  ChatGPT中文指南    |       |   gitlab    |
|   探讨了ChatGPT在自然语言处理中的应用、优势、限制以及未来发展方向   |   强调了在使用该技术时的伦理道德考量和提示工程技术。    |    paper   |
|大型语言模型相关文献资源列表||github|
|大型语言模型文献综述--中文版||github|
|ChatGPT 相关资源大列表||github|
|Pre-Training to Learn in Context||paper|
|Langchain架构图||image|
|LLM开发人员都应该知道的数字||github|
|大语言模型如何构建强大的复杂推理能力||blog|
|LLMs九层妖塔|分享打怪(ChatGLM、Chinese-LLaMA-Alpaca、MiniGPT-4、FastChat、LLaMA、gpt4all等)实战与经验|github|