---
repo: huggingface/transformers
readme_filename: huggingface_transformers_README.md
stars: 146142
forks: 29471
watchers: 146142
contributors_count: 436
license: Apache-2.0
---
  








  








  


English |
简体中文 |
繁體中文 |
한국어 |
Español |
日本語 |
हिन्दी |
Русский |
Рortuguês |
తెలుగు |
Français |
Deutsch |
Tiếng Việt |
العربية |
اردو |

  

State-of-the-art pretrained models for inference and training
  


  
Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer
vision, audio, video, and multimodal model, for both inference and training.  
It centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the
pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training
frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),
and adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.  
We pledge to help support new state-of-the-art models and democratize their usage by having their model definition be
simple, customizable, and efficient.  
There are over 1M+ Transformers model checkpoints on the Hugging Face Hub you can use.  
Explore the Hub today to find a model and use Transformers to help you get started right away.